{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94bb8b20-6978-4b7d-a489-48020ee475b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"assets/Day5.png\" alt=\"Databricks 14 Days AI Challenge - Day 05\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "## DAY 5 (13/01/26) â€“ Delta Lake Advanced\n",
    "\n",
    "### ðŸ“š Learning Objectives\n",
    "Yesterday we built the foundation. Today, we master the features that make Delta Lake \"production-grade\":\n",
    "* **Time Travel:** Querying older versions of data for auditing or rollback (like Git for data). \n",
    "* **Incremental MERGE:** Efficiently updating changing data (SCD Type 1/2) without rewriting the full table.\n",
    "* **Optimization:** Using `OPTIMIZE` and `ZORDER` to speed up queries by organizing file layout.\n",
    "* **Maintenance:** Using `VACUUM` to clean up stale files and manage storage costs.\n",
    "\n",
    "### ðŸš€ Strategy: The \"Audit & Optimize\" Pipeline\n",
    "1.  **Simulate Updates:** Create a batch of \"corrected\" data (e.g., updating incorrect prices) and new records.\n",
    "2.  **Upsert (Merge):** Apply these changes to our Silver table using the `MERGE` command.\n",
    "3.  **Audit (Time Travel):** Compare the table \"Before\" vs. \"After\" to verify the changes.\n",
    "4.  **Tune:** Reorganize the physical file layout to make future queries faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33b61604-76a9-4774-b11a-e2238d4d72a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Setup & Load\n",
    "**Task**: Define paths and initialize the Delta Table object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3645a17-9be6-4d71-a6fb-59ddd3276581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "\n",
    "# Define the path to our existing Silver table (created in Day 4)\n",
    "delta_path = \"/Volumes/workspace/ecommerce/ecommerce_data/delta/events_bronze\"\n",
    "\n",
    "# Initialize DeltaTable object (Entry point for advanced DML operations)\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "print(f\"âœ… Delta Table loaded from: {delta_path}\")\n",
    "print(f\"   Current Version: {delta_table.history().select('version').first()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaaf2491-fbe2-4c0a-981f-a98f9112dc77",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inspect Delta Table Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the schema and sample data of the target Delta table\n",
    "df_target = spark.read.format(\"delta\").load(delta_path)\n",
    "df_target.printSchema()\n",
    "df_target.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b99a53c-8410-4fd8-821f-af9d6aee2fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Incremental MERGE (The Upsert)\n",
    "**Task**: Implement incremental MERGE. \n",
    "\n",
    "**Concept**: We simulate a scenario where we found \"Data Errors\" in yesterday's batch (e.g., negative prices) and need to fix them while simultaneously adding new real-time events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514d658e-aa31-409e-9452-0b34199f3946",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 5"
    }
   },
   "outputs": [],
   "source": [
    "# 1. CREATE \"UPDATES\" DATASET\n",
    "# Scenario: \n",
    "# - User 563509121 had a transaction with price 100. We need to correct it to 150.\n",
    "# - We also have 1 completely new incoming event.\n",
    "\n",
    "updates_data = [\n",
    "    # Existing Row (Update Case): Matching keys (user_id, event_time, product_id)\n",
    "    (\"2019-11-01 00:00:10\", \"purchase\", 1004258, 563509121, \"electronics.smartphone\", \"apple\", 150.00, \"Nov\"),\n",
    "    # New Row (Insert Case)\n",
    "    (\"2020-01-01 10:00:00\", \"purchase\", 9999999, 111111111, \"test.category\", \"test_brand\", 50.00, \"Jan\")\n",
    "]\n",
    "\n",
    "# Create DataFrame with same schema as target\n",
    "columns = [\"event_time\", \"event_type\", \"product_id\", \"user_id\", \"category_code\", \"brand\", \"price\", \"month\"]\n",
    "df_updates = spark.createDataFrame(updates_data, columns)\n",
    "\n",
    "# Add missing columns to match target schema\n",
    "from pyspark.sql.functions import lit\n",
    "if \"category_id\" not in df_updates.columns:\n",
    "    df_updates = df_updates.withColumn(\"category_id\", lit(None).cast(\"long\"))\n",
    "if \"user_session\" not in df_updates.columns:\n",
    "    df_updates = df_updates.withColumn(\"user_session\", lit(None).cast(\"string\"))\n",
    "\n",
    "# Reorder columns to match target table\n",
    "target_columns = [\"event_time\", \"event_type\", \"product_id\", \"category_id\", \"category_code\", \"brand\", \"price\", \"user_id\", \"user_session\", \"month\"]\n",
    "df_updates = df_updates.select(target_columns)\n",
    "\n",
    "print(\"--- Incoming Updates Batch ---\")\n",
    "df_updates.show()\n",
    "\n",
    "# 2. PERFORM MERGE\n",
    "# Logic: Match on Composite Key (User + Time + Product)\n",
    "delta_table.alias(\"target\").merge(\n",
    "    df_updates.alias(\"source\"),\n",
    "    \"\"\"\n",
    "    target.user_id = source.user_id AND \n",
    "    target.event_time = source.event_time AND \n",
    "    target.product_id = source.product_id\n",
    "    \"\"\"\n",
    ").whenMatchedUpdateAll( # If key exists, update all columns (Corrects the price)\n",
    ").whenNotMatchedInsertAll( # If key doesn't exist, insert new row\n",
    ").execute()\n",
    "\n",
    "print(\"âœ… MERGE Operation Complete (Upsert executed).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0c72488-7965-4a65-87d7-035cc0af2057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Time Travel (Querying History)\n",
    "* **Task**: Query historical versions. \n",
    "* **Concept**: We will verify our update by querying the table at version 0 (original) and version 1 (after merge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f34f2b-0b17-4003-bd03-e61618070bac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 8"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# STEP 3: TIME TRAVEL AUDIT\n",
    "# Goal: Compare the \"Before\" (v0) and \"After\" (vLatest) states.\n",
    "# Note: We add safety checks because the user might not have existed in v0!\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. READ LATEST VERSION (Current State)\n",
    "df_latest = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Safe extraction helper\n",
    "def get_price_safe(df, uid, version_label):\n",
    "    row = df.filter(col(\"user_id\") == uid).select(\"price\").first()\n",
    "    if row:\n",
    "        return row[0]\n",
    "    else:\n",
    "        return \"NOT FOUND (Did not exist)\"\n",
    "\n",
    "# Target User ID (The one we updated/inserted in the previous step)\n",
    "target_user_id = 563509121\n",
    "\n",
    "# Get Prices\n",
    "price_latest = get_price_safe(df_latest, target_user_id, \"Latest\")\n",
    "\n",
    "# 2. READ OLD VERSION (Time Travel to Version 0)\n",
    "# We use 'versionAsOf' to travel back to the very first write\n",
    "df_old = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n",
    "price_old = get_price_safe(df_old, target_user_id, \"v0\")\n",
    "\n",
    "print(f\"ðŸ•µï¸ TIME TRAVEL AUDIT for User {target_user_id}:\")\n",
    "print(f\"   Original Price (v0):      {price_old}\")\n",
    "print(f\"   Updated Price  (vLatest): {price_latest}\")\n",
    "\n",
    "# 3. VISUALIZE HISTORY\n",
    "# This shows the transaction log (who changed what and when)\n",
    "display(delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea2337c7-2cb0-4248-bb67-a6fcb3d6d759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Performance Optimization (Z-ORDER)\n",
    "**Task**: Optimize tables. \n",
    "\n",
    "**Concept**: Z-Ordering is a technique to co-locate related information in the same set of files. Since we often filter by event_type (e.g., \"purchase\") and brand, clustering data by these columns allows Spark to skip huge chunks of irrelevant files during queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052d507d-47a7-4add-8bfe-b1a6d4f14af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run OPTIMIZE and ZORDER BY high-cardinality query columns\n",
    "# Note: This triggers a Spark job that compacts small files and reorganizes data\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE delta.`{delta_path}`\n",
    "    ZORDER BY (event_type, brand)\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… Optimization Complete. Small files compacted and Z-Ordered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35eea03f-b5a1-400f-b288-bc15c192ea84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Storage Cleanup (VACUUM)\n",
    "**Task**: Clean old files. \n",
    "\n",
    "**Concept**: When we \"overwrite\" or \"update\" data, Delta doesn't delete the old file immediately (to allow Time Travel). VACUUM permanently deletes files no longer referenced by a Delta log older than the retention period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cad6db-aef1-4ee8-8897-2b40044d0e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# âš ï¸ SAFETY CHECK: \n",
    "# By default, Delta prevents vacuuming files newer than 7 days (168 hours) to prevent \n",
    "# corrupting long-running queries. We will respect this safe default.\n",
    "\n",
    "# Run VACUUM with default retention\n",
    "# dryRun=True allows us to see what WOULD be deleted without actually deleting it\n",
    "deleted_files = delta_table.vacuum(retentionHours=168)\n",
    "\n",
    "print(\"âœ… VACUUM completed (Retention: 7 Days).\")\n",
    "# Note: If you really need to delete immediate files (DEV only), you would use:\n",
    "# spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "# delta_table.vacuum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d66fd5e1-3b44-441b-9cb2-95adc0722f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ§  Key Learnings & Takeaways\n",
    "* **Merge > Rewrite:** We used `MERGE` to handle \"late-arriving updates\" efficiently. This is critical for Change Data Capture (CDC) pipelines.\n",
    "* **Time Travel is Native:** We didn't need to build a separate history table. Delta Lake maintained the \"Before\" state automatically, allowing us to audit changes easily.\n",
    "* **Z-Ordering:** We learned that `OPTIMIZE` isn't just about file sizeâ€”it's about *sorting* data (Z-Ordering) to maximize \"Data Skipping\" performance during reads.\n",
    "* **Storage Hygiene:** `VACUUM` is essential for controlling cloud costs, but it must be used carefully (it removes the ability to Time Travel back past the retention period)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
